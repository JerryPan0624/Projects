{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/faculty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamWeightDecay, pipeline, create_optimizer, Trainer, TrainingArguments\n",
    "from transformers import DefaultDataCollator\n",
    "from datasets import load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Flag to let us know if we're currently running in Google Colab or locally\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_auto = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer_auto.pad_token = tokenizer_auto.eos_token\n",
    "auto_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer_auto.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(data):\n",
    "    tokens = tokenizer_auto(data[\"sentences\"], padding=\"max_length\", truncation=True, max_length=70)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(text):\n",
    "    text[\"labels\"] = text[\"input_ids\"].copy()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arcane= pd.read_csv('/project/nlp_text_generation_sentences_data_arcane_champions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arcane_train_val, test_set_arcane = train_test_split(data_arcane, test_size=0.2, random_state=200)\n",
    "\n",
    "data_arcane_train_val_set = Dataset.from_pandas(data_arcane_train_val)\n",
    "data_arcane_train_val_set = data_arcane_train_val_set.train_test_split(shuffle = True, seed = 200, test_size=0.25)\n",
    "train_arcane = data_arcane_train_val_set[\"train\"]\n",
    "val_arcane = data_arcane_train_val_set[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"656c70e7-f782-4c02-abd6-516be2e9c66a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"656c70e7-f782-4c02-abd6-516be2e9c66a\")) {                    Plotly.newPlot(                        \"656c70e7-f782-4c02-abd6-516be2e9c66a\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"variable=0<br>Sentences Length (words)=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"0\",\"nbinsx\":400,\"offsetgroup\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[8,37,34,11,23,22,18,15,8,6,35,27,34,8,4,21,15,10,31,24,52,6,20,4,26,32,8,3,16,14,36,29,10,12,19,22,4,2,11,8,11,22,18,45,16,13,19,35,14,1,10,21,16,8,30,20,13,15,18,20,25,23,22,27,13,15,12,6,10,3,4,23,18,8,21,21,21,11,27,13,24,26,6,7,20,12,5,19,14,18,22,38,12,25,37,31,5,17,15,16,7,19,25,2,10,17,32,23,25,37,18,18,23,8,3,14,29,23,17,17,25,18,23,53,12,32,19,6,14,7,7,5,13,18,20,17,15,19,19,29,11,14,11,12,4,6,19,4,24,8,16,10,3,31,28,21,15,1,12,25,15,16,17,18,22,19,17,22,33,21,1,29,23,57,16,34,22,14,22,46,11,10,9,13,28,23,26,5,46,29,7,29,14,13,24,6,28,5,18,26,18,20,29,9,12,16,18,28,8,5,14,25,4,14,30,32,26,11,22,19,1,30,26,9,25,21,5,12,29,27,20,17,19,7,20,12,22,6,16,16,32,31,19,14,17,12,3,2,41,8,19,3,23,9,49,15,29,28,13,28,23,8,11,18,29,5,9,7,23,23,19,16,21,37,27,4,24,6,14,19,18,20,19,23,34,6,21,32,23,37,35,13,14,48,30,6,9,7,25,16,37,24,22,4,14,9,11,18,18,2,17,23,36,14,27,14,22,6,9,11,11,22,17,17,16,22,5,18,39,73,1,1,36,24,22,9,9,15,31,8,8,9,10,3,18,34,8,14,18,24,17,10,39,10,18,17,32,18,17,23,7,15,6,4,9,14,27,17,19,38,10,18,9,1,28,43,20],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"},{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"fillcolor\":\"rgba(255,255,255,0)\",\"hoveron\":\"points\",\"hovertemplate\":\"variable=0<br>Sentences Length (words)=%{x}<extra></extra>\",\"jitter\":0,\"legendgroup\":\"0\",\"line\":{\"color\":\"rgba(255,255,255,0)\"},\"marker\":{\"color\":\"#636efa\",\"symbol\":\"line-ns-open\"},\"name\":\"0\",\"offsetgroup\":\"0\",\"showlegend\":false,\"x\":[8,37,34,11,23,22,18,15,8,6,35,27,34,8,4,21,15,10,31,24,52,6,20,4,26,32,8,3,16,14,36,29,10,12,19,22,4,2,11,8,11,22,18,45,16,13,19,35,14,1,10,21,16,8,30,20,13,15,18,20,25,23,22,27,13,15,12,6,10,3,4,23,18,8,21,21,21,11,27,13,24,26,6,7,20,12,5,19,14,18,22,38,12,25,37,31,5,17,15,16,7,19,25,2,10,17,32,23,25,37,18,18,23,8,3,14,29,23,17,17,25,18,23,53,12,32,19,6,14,7,7,5,13,18,20,17,15,19,19,29,11,14,11,12,4,6,19,4,24,8,16,10,3,31,28,21,15,1,12,25,15,16,17,18,22,19,17,22,33,21,1,29,23,57,16,34,22,14,22,46,11,10,9,13,28,23,26,5,46,29,7,29,14,13,24,6,28,5,18,26,18,20,29,9,12,16,18,28,8,5,14,25,4,14,30,32,26,11,22,19,1,30,26,9,25,21,5,12,29,27,20,17,19,7,20,12,22,6,16,16,32,31,19,14,17,12,3,2,41,8,19,3,23,9,49,15,29,28,13,28,23,8,11,18,29,5,9,7,23,23,19,16,21,37,27,4,24,6,14,19,18,20,19,23,34,6,21,32,23,37,35,13,14,48,30,6,9,7,25,16,37,24,22,4,14,9,11,18,18,2,17,23,36,14,27,14,22,6,9,11,11,22,17,17,16,22,5,18,39,73,1,1,36,24,22,9,9,15,31,8,8,9,10,3,18,34,8,14,18,24,17,10,39,10,18,17,32,18,17,23,7,15,6,4,9,14,27,17,19,38,10,18,9,1,28,43,20],\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Sentences Length (words)\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,0.7326],\"title\":{\"text\":\"count\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0],\"matches\":\"x\",\"showticklabels\":false,\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7426,1.0],\"matches\":\"y2\",\"showticklabels\":false,\"showline\":false,\"ticks\":\"\",\"showgrid\":false},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('656c70e7-f782-4c02-abd6-516be2e9c66a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [len(x.split()) for x in data_arcane_train_val[\"sentences\"]]\n",
    "px.histogram(sentences, nbins=400, marginal=\"rug\", labels={\"value\":\"Sentences Length (words)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the tokenizer in batch mode and drop all the columns except the tokenization result\n",
    "train_token_arcane = train_arcane.map(tokenization, batched = True, remove_columns=[\"Unnamed: 0\"], num_proc=10)\n",
    "val_token_arcane = val_arcane.map(tokenization, batched = True, remove_columns=[\"Unnamed: 0\"], num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the labels column using map()\n",
    "lm_train_arcane = train_token_arcane.map(create_labels, batched=True, num_proc=10)\n",
    "lm_val_arcane = val_token_arcane.map(create_labels, batched=True, num_proc=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = [0.001, 0.0001, 0.00005] #\n",
    "# weight_decays = [0.01, 0.005, 0.001] #, \n",
    "# epochs= [1,2,3]\n",
    "# best_result_arc= None\n",
    "# best_loss_arc=  float('inf')\n",
    "\n",
    "# for epoch in epochs:\n",
    "#     for lr in learning_rates:\n",
    "#         for wd in weight_decays:\n",
    "#             auto_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "#             training_args_arc = TrainingArguments(output_dir=f'./results_arc', \n",
    "#                                               num_train_epochs= epoch, \n",
    "#                                               logging_strategy=\"epoch\",\n",
    "#                                               #logging_steps=20, \n",
    "#                                               save_strategy=\"epoch\",\n",
    "#                                               #save_steps=20,\n",
    "#                                               evaluation_strategy='epoch',\n",
    "#                                               #eval_steps=20,\n",
    "#                                               learning_rate= lr ,\n",
    "#                                               per_device_train_batch_size=1, \n",
    "#                                               per_device_eval_batch_size=1,\n",
    "#                                               #warmup_steps=5, \n",
    "#                                               weight_decay=wd, \n",
    "#                                               logging_dir='./logs',\n",
    "#                                               load_best_model_at_end=True)\n",
    "\n",
    "\n",
    "#             trainer_arc = Trainer(model=auto_model,  \n",
    "#                               args=training_args_arc, \n",
    "#                               train_dataset=lm_train_arcane, \n",
    "#                               eval_dataset=lm_val_arcane)\n",
    "#             #                    compute_metrics=compute_metrics)\n",
    "\n",
    "\n",
    "#             trainer_arc.train()   \n",
    "\n",
    "#             result_arc = trainer_arc.evaluate(eval_dataset=lm_val_arcane)\n",
    "\n",
    "#             # Check if the current result is the best so far\n",
    "#             if result_arc['eval_loss'] < best_loss_arc:\n",
    "#                 best_loss_arc = result_arc['eval_loss']\n",
    "#                 best_result_arc = {'epoch': epoch, 'learning_rate': lr, 'weight_decay': wd, 'eval_loss': best_loss_arc}                \n",
    "#                 trainer_arc.save_model(f'./best_model_arc')\n",
    "#             #reset the model for the next loop of training     \n",
    "# print('Best result:', best_result_arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result={'epoch': 2, 'learning_rate': 0.00005, 'weight_decay': 0.001, 'eval_loss': 1.278050}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity =math.exp(best_result_arc['eval_loss'])\n",
    "# print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "/opt/anaconda/envs/Python3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning:\n",
      "\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 25:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.538000</td>\n",
       "      <td>1.278050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.048000</td>\n",
       "      <td>1.294168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auto_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "training_args_arc = TrainingArguments(output_dir=f'./results_arc', \n",
    "                                  num_train_epochs= 2, \n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  #logging_steps=20, \n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  #save_steps=20,\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  #eval_steps=20,\n",
    "                                  learning_rate= 0.00005 ,\n",
    "                                  per_device_train_batch_size=1, \n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  #warmup_steps=5, \n",
    "                                  weight_decay=0.001, \n",
    "                                  logging_dir='./logs',\n",
    "                                  load_best_model_at_end=True)\n",
    "\n",
    "\n",
    "trainer_arc = Trainer(model=auto_model,  \n",
    "                  args=training_args_arc, \n",
    "                  train_dataset=lm_train_arcane, \n",
    "                  eval_dataset=lm_val_arcane)\n",
    "#                    compute_metrics=compute_metrics)\n",
    "\n",
    "\n",
    "trainer_arc.train()\n",
    "trainer_arc.save_model(f'./best_model_arc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [95/95 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5896337731626167\n"
     ]
    }
   ],
   "source": [
    "result_arc = trainer_arc.evaluate(eval_dataset=lm_val_arcane)\n",
    "perplexity =math.exp(result_arc['eval_loss'])\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_arc = AutoModelForCausalLM.from_pretrained(f'./best_model_arc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=best_model_arc,\n",
    "    tokenizer=tokenizer_auto,\n",
    "    framework=\"pt\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    min_length= 100,\n",
    "    min_new_tokens=0, \n",
    "    max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A strong, lawful kingdom with a prestigious military history and strong military reputation was quickly growing and increasingly unlikely to survive until something new was discovered to improve the system of their war. For some time the army had been on a course of great importance as it developed from a more expensive military automatons to a more effective defensive technology such as a powerful and versatile warhorn and a variety of powerful new technology to make them even more versatile and useful in battle! This trend culminated in a long string of scandals that led some of the most trusted and respected'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"A strong, lawful kingdom with a prestigious military history\"\n",
    "text_generator(test_sentence)[0][\"generated_text\"].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU for test set (examining the quality of the generated text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arcane_text_generator_set = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=best_model_arc,\n",
    "    tokenizer=tokenizer_auto,\n",
    "    framework=\"pt\",\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    min_length= 40,\n",
    "    max_length=200)\n",
    "\n",
    "def generate_text_set(test_sentence):\n",
    "    generated_text=arcane_text_generator_set(test_sentence)[0][\"generated_text\"].replace(\"\\n\", \" \")\n",
    "    generated_text = generated_text[len(test_sentence):].lstrip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and a long history of alliances with allies all across the empire it became an even more powerful weapon than ever before when a rogue and powerful criminal mastermind sought the secrets of the city of Zaun'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"A strong, lawful kingdom with a prestigious military history\"\n",
    "generate_text_set(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_arcane['label']= test_set_arcane.shift(-1)[\"sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_set_arcane= test_set_arcane.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_arcane['generated']= test_set_arcane['sentences'].apply(lambda x : generate_text_set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419    dripping from the temples and temples in Zaun ...\n",
      "428    crystal and if he has plans to get one of his ...\n",
      "115    as well as Jayce had seen his skills grow over...\n",
      "287    % chance of it being hit with a baseball ball ...\n",
      "237    but would not be taken for granted as a necess...\n",
      "                             ...                        \n",
      "47     but instead to save some of her young comrades...\n",
      "245    and technology. During his time working with t...\n",
      "323    though it might be better to use a single crys...\n",
      "226    in order to enhance her strength and strength ...\n",
      "264                                           from decay\n",
      "Name: generated, Length: 93, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test_set_arcane['generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
    "def bleu(ref, gen):\n",
    "    ''' \n",
    "    calculate pair wise bleu score. uses nltk implementation\n",
    "    Args:\n",
    "        references : a list of reference sentences \n",
    "        candidates : a list of candidate(generated) sentences\n",
    "    Returns:\n",
    "        bleu score(float)\n",
    "    '''\n",
    "    ref_bleu = []\n",
    "    gen_bleu = []\n",
    "    for l in gen:\n",
    "        gen_bleu.append(l.split())\n",
    "    for i,l in enumerate(ref):\n",
    "        ref_bleu.append([l.split()])\n",
    "    cc = SmoothingFunction()\n",
    "    score_bleu = corpus_bleu(ref_bleu, gen_bleu, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=cc.method4)\n",
    "    return score_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLEU_arcane= bleu(test_set＿arcane['label'].tolist(),test_set_arcane['generated'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028013918585460765\n"
     ]
    }
   ],
   "source": [
    "print(BLEU_arcane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU score on the new paragraph (holistic way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcane_para= \"\"\" \n",
    "Two sisters. Two cities. One discovery that will change the world forever. In the cities of Piltover and Zaun, unrest stirs as inventors and thieves, politicians and crime lords chafe against the constraints of a society torn asunder.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"\"\" \n",
    "As dissent reaches a fever pitch, two sisters steal an artifact of untold power. Discovery and danger intertwine as heroes are born and bonds are broken. Will this power change the world, or lead it to ruin? This is the world of Arcane. Nestled at the heart of the continent, Piltover stands on the precipice of progress. The invention of hextech shines with subtle promise of a glorious future, and two brilliant scientists lead the charge. Such potential is not without cost, and failure could prove devastating as politicians vie for a power they barely understand. As unrest stirs in the Undercity and upheaval looms, the people of Piltover must protect their future… no matter the cost. \n",
    "In the shadows of progress, far below the ivory towers of Piltover, the Undercity waits. The air there is thick with shouts and smoke, and each alleyway promises danger, desire...or both. But at its heart, this sister city thrums with hope, ingenuity, and an unbreakable spirit that neither greedy chembarons nor aggressive enforcers could ever take away. As the age of hextech looms, her citizens no longer see themselves as Topsiders. Instead, they look up with desperate determination and dream of something more.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcane_text_generator_para = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=best_model_arc,\n",
    "    tokenizer=tokenizer_auto,\n",
    "    framework=\"pt\",\n",
    "    temperature=1.0,\n",
    "    top_k=120,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    min_length= len(target.split(\" \"))+len(arcane_para.split(\" \")),\n",
    "    min_new_tokens=0, \n",
    "    max_new_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_para(test_sentence):\n",
    "    generated_text=arcane_text_generator_para(test_sentence)[0][\"generated_text\"].replace(\"\\n\", \" \")\n",
    "    generated_text = generated_text[len(test_sentence):].lstrip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.9/site-packages/transformers/generation/utils.py:1201: UserWarning:\n",
      "\n",
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_arcane=generate_text_para(arcane_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_arcane=\"\"\"\n",
    "Earning nothing is always a struggle and a new path that they know but will solve the problems from a better point of view a better one of Zaun itself. But in Piltover today the city itself is a battle for undertake a new path the first step in transforming mankind into a more agile and more versatile beast To the north the new academy Zaun has developed to help them navigate the city streets as they navigate the increasingly empty streets of Piltover and learn from them what it all means to be an engineer or a true lawyer and what it means to solve dilemmas the first time their efforts to learn from Piltovers parents parents could be futile at best. The cities of Piltover and Zaun have been in upheaval for a long time and the warring factions in Piltovers are yet to resolve their differences peacefully but soon something will change in Zaun and a new order of Zaunian Zaun will soon be at stake \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Earning nothing is always a struggle and a new path that they know but will solve the problems from a better point of view a better one of Zaun itself. But in Piltover today the city itself is a battle for undertake a new path the first step in transforming mankind into a more agile and more versatile beast To the north the new academy Zaun has developed to help them navigate the city streets as they navigate the increasingly empty streets of Piltover and learn from them what it all means to be an engineer or a true lawyer and what it means to solve dilemmas the first time their efforts to learn from Piltovers parents parents could be futile at best. The cities of Piltover and Zaun have been in upheaval for a long time and the warring factions in Piltovers are yet to resolve their differences peacefully but soon something will change in Zaun and a new order of Zaunian Zaun will soon be at stake \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_arcane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 3.731163506456045e-155\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the paragraphs into lists of words\n",
    "ref_tokens = nltk.word_tokenize(target.lower())\n",
    "gen_tokens = nltk.word_tokenize(generated_arcane.lower())\n",
    "\n",
    "# Calculate the BLEU score with 4-gram precision\n",
    "bleu_score = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(embedding_1:np.array, embedding_2:np.array) -> float:\n",
    "  dot_product = np.dot(embedding_1, embedding_2)\n",
    "  norm1 = np.linalg.norm(embedding_1)\n",
    "  norm2 = np.linalg.norm(embedding_2)\n",
    "  cosine_similarity = dot_product / (norm1 * norm2)\n",
    "  return cosine_similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83365035\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode the sentences using the BERT tokenizer\n",
    "encoded_input_1 = tokenizer(target, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Pass the encoded input through the BERT model to obtain the document embeddings\n",
    "outputs = model(**encoded_input_1)\n",
    "\n",
    "document_embeddings_1 = outputs.last_hidden_state[:, 0, :]\n",
    "document_embeddings_1 = document_embeddings_1.detach().numpy()\n",
    "\n",
    "# Encode the sentences using the BERT tokenizer\n",
    "encoded_input_2 = tokenizer(generated_arcane, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Pass the encoded input through the BERT model to obtain the document embeddings\n",
    "outputs = model(**encoded_input_2)\n",
    "\n",
    "document_embeddings_2 = outputs.last_hidden_state[:, 0, :]\n",
    "document_embeddings_2 = document_embeddings_2.detach().numpy()\n",
    "similarity_bert = get_similarity(document_embeddings_1[0], document_embeddings_2[0])\n",
    "print(similarity_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling comparing similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.83918697\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the two text passages as a list of strings\n",
    "\n",
    "\n",
    "# preprocess the text by tokenizing and removing stop words\n",
    "texts = [gensim.utils.simple_preprocess(story) for story in [target, generated_arcane]]\n",
    "\n",
    "# create a dictionary of unique terms in the text\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# convert the text into a bag-of-words representation\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# perform LDA topic modeling on the corpus\n",
    "numpy.random.seed(58) \n",
    "lda_model = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "# extract the topic distributions for each document\n",
    "doc1_topics = lda_model.get_document_topics(corpus[0])\n",
    "doc2_topics = lda_model.get_document_topics(corpus[1])\n",
    "\n",
    "# compare the topic distributions using cosine similarity\n",
    "similarity = similarities.MatrixSimilarity([doc1_topics])\n",
    "cos_sim = similarity[doc2_topics][0]\n",
    "\n",
    "# print the similarity score\n",
    "print(\"Cosine similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
